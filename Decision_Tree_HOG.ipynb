{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
<<<<<<< HEAD
      "provenance": [],
      "authorship_tag": "ABX9TyO7WFRyGrouFf8TuIlnJP3k",
      "include_colab_link": true
=======
      "provenance": []
>>>>>>> ab4aa8f05 (Updated Decision Tree HOG notebook)
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 24,
      "metadata": {
        "id": "Ipsovb1x28BV"
      },
      "outputs": [],
=======
>>>>>>> ab4aa8f05 (Updated Decision Tree HOG notebook)
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
<<<<<<< HEAD
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load your CSV files\n",
        "train_data = pd.read_csv('HOG_Training.csv')\n",
        "test_data = pd.read_csv('HOG_Testing.csv')\n"
      ],
      "metadata": {
        "id": "gwN8Goj2-bzf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
=======
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 1. Load your CSV files\n",
        "train_data = pd.read_csv('HOG_Training.csv')\n",
        "test_data = pd.read_csv('HOG_Testing.csv')\n",
        "\n",
        "\n",
        "# 2. Separate features and labels\n",
>>>>>>> ab4aa8f05 (Updated Decision Tree HOG notebook)
        "X_train = train_data.iloc[:, 1:]  # All columns except the first one\n",
        "y_train = train_data.iloc[:, 0]   # First column as target\n",
        "X_test = test_data.iloc[:, 1:]    # All columns except the first one\n",
        "y_test = test_data.iloc[:, 0]     # First column as target\n",
        "\n",
        "# 3. Handle unseen labels in test set\n",
        "test_mask = y_test.isin(y_train.unique())\n",
        "X_test_filtered = X_test[test_mask]\n",
        "y_test_filtered = y_test[test_mask]\n",
        "\n",
        "# 4. Encode the class labels\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test_filtered)\n",
        "\n",
        "# 5. Handle missing values (NaN) using imputation\n",
        "print(\"Handling missing values...\")\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test_filtered)\n",
        "\n",
        "# 6. Train a baseline model first\n",
        "print(\"Training baseline decision tree...\")\n",
        "baseline_dt = DecisionTreeClassifier(random_state=42)\n",
        "baseline_dt.fit(X_train_imputed, y_train_encoded)\n",
        "baseline_pred = baseline_dt.predict(X_test_imputed)\n",
        "baseline_accuracy = accuracy_score(y_test_encoded, baseline_pred)\n",
        "print(f\"Baseline Decision Tree Accuracy: {baseline_accuracy:.4f}\")\n",
        "\n",
        "# 7. Try a few strategically selected hyperparameter combinations\n",
        "print(\"Testing specific hyperparameter combinations...\")\n",
        "models = [\n",
        "    (\"Default + Max Depth 10\", DecisionTreeClassifier(max_depth=10, random_state=42)),\n",
        "    (\"Default + Max Depth 20\", DecisionTreeClassifier(max_depth=20, random_state=42)),\n",
        "    (\"Entropy + Max Depth 20\", DecisionTreeClassifier(criterion='entropy', max_depth=20, random_state=42)),\n",
        "    (\"Min Samples Split 10\", DecisionTreeClassifier(min_samples_split=10, random_state=42)),\n",
        "    (\"Min Samples Leaf 5\", DecisionTreeClassifier(min_samples_leaf=5, random_state=42)),\n",
        "    (\"Balanced Weights\", DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
        "    (\"Combined Parameters\", DecisionTreeClassifier(\n",
        "        criterion='entropy',\n",
        "        max_depth=20,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=2,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ))\n",
        "]\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = baseline_accuracy\n",
        "best_name = \"Baseline\"\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train_imputed, y_train_encoded)\n",
        "    y_pred = model.predict(X_test_imputed)\n",
        "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "        best_name = name\n",
        "\n",
        "print(f\"\\nBest model: {best_name} with accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# 8. Print classification report for the best model\n",
        "if best_model is not None:\n",
        "    y_pred = best_model.predict(X_test_imputed)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test_encoded, y_pred))\n",
        "\n",
        "    # Feature importance analysis\n",
        "    feature_importances = best_model.feature_importances_\n",
        "    feature_indices = np.argsort(feature_importances)[::-1]\n",
        "    print(\"\\nTop 10 most important features:\")\n",
        "    for i in range(min(10, len(feature_indices))):\n",
        "        print(f\"Feature {feature_indices[i]}: {feature_importances[feature_indices[i]]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz-BF__DzXx0",
        "outputId": "629ff388-55c3-4d7c-801a-cdf5500fcf52"
      },
<<<<<<< HEAD
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mask = y_test.isin(y_train.unique())\n",
        "X_test_filtered = X_test[test_mask]\n",
        "y_test_filtered = y_test[test_mask]\n"
      ],
      "metadata": {
        "id": "7L56-8TlBU_s"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Encode the class labels\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test_filtered)"
      ],
      "metadata": {
        "id": "SQ1J-0lmE5pU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Handle missing values (NaN) using imputation\n",
        "print(\"Handling missing values...\")\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvazXsdxFaJq",
        "outputId": "8cdb1714-2271-4662-b500-dafc9e802830"
      },
      "execution_count": 29,
=======
      "execution_count": null,
>>>>>>> ab4aa8f05 (Updated Decision Tree HOG notebook)
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
<<<<<<< HEAD
            "Handling missing values...\n"
=======
            "Handling missing values...\n",
            "Training baseline decision tree...\n",
            "Baseline Decision Tree Accuracy: 0.5479\n",
            "Testing specific hyperparameter combinations...\n",
            "Default + Max Depth 10 Accuracy: 0.5143\n",
            "Default + Max Depth 20 Accuracy: 0.5446\n",
            "Entropy + Max Depth 20 Accuracy: 0.5840\n",
            "Min Samples Split 10 Accuracy: 0.5452\n",
            "Min Samples Leaf 5 Accuracy: 0.5581\n",
            "Balanced Weights Accuracy: 0.5515\n",
            "Combined Parameters Accuracy: 0.5819\n",
            "\n",
            "Best model: Entropy + Max Depth 20 with accuracy: 0.5840\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.43      0.46       144\n",
            "           1       0.67      0.64      0.66       164\n",
            "           2       0.87      0.72      0.79       152\n",
            "           3       0.81      0.78      0.80       164\n",
            "           4       0.71      0.66      0.68       164\n",
            "           5       0.68      0.35      0.46       150\n",
            "           6       0.00      0.00      0.00         0\n",
            "           7       0.43      0.37      0.40        99\n",
            "           8       0.60      0.60      0.60       144\n",
            "           9       0.65      0.47      0.55       156\n",
            "          10       0.62      0.51      0.56       164\n",
            "          11       0.00      0.00      0.00         0\n",
            "          12       0.00      0.00      0.00         0\n",
            "          13       0.47      0.46      0.46       160\n",
            "          14       0.64      0.59      0.61       164\n",
            "          15       0.93      0.96      0.94       246\n",
            "          16       0.00      0.00      0.00         0\n",
            "          17       0.00      0.00      0.00         0\n",
            "          18       0.50      0.28      0.36       234\n",
            "          19       0.00      0.00      0.00         0\n",
            "          20       0.00      0.00      0.00         0\n",
            "          21       0.40      0.36      0.38       150\n",
            "          22       0.00      0.00      0.00         0\n",
            "          23       0.52      0.49      0.51       246\n",
            "          24       0.86      0.70      0.77       246\n",
            "          25       0.74      0.78      0.76       225\n",
            "          26       0.70      0.69      0.69       157\n",
            "\n",
            "    accuracy                           0.58      3329\n",
            "   macro avg       0.46      0.40      0.42      3329\n",
            "weighted avg       0.66      0.58      0.62      3329\n",
            "\n",
            "\n",
            "Top 10 most important features:\n",
            "Feature 227: 0.1188\n",
            "Feature 87: 0.0746\n",
            "Feature 2: 0.0671\n",
            "Feature 108: 0.0306\n",
            "Feature 240: 0.0304\n",
            "Feature 249: 0.0273\n",
            "Feature 172: 0.0268\n",
            "Feature 11: 0.0266\n",
            "Feature 78: 0.0263\n",
            "Feature 246: 0.0209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
>>>>>>> ab4aa8f05 (Updated Decision Tree HOG notebook)
          ]
        }
      ]
    }
  ]
}